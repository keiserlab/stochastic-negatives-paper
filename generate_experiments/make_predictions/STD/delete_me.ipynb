{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from itertools import izip\n",
    "from common.plot_fcns import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0,'/srv/home/nmew/myprojects/clean-neural-nets/')\n",
    "from common.h5py_loading import load_target_map, align_target_maps\n",
    "from common.chembl_export_data_loader import DrugMatrixDataLoader\n",
    "from common.h5py_data_loader import H5pyDataLoader\n",
    "from lasagne_nn.run_nn import get_predictions_of_knowns, get_network_from_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset can be 'test', 'train', 'val' or 'drugmatrix'# datase \n",
    "def predictions_knowns_from_trained_network_and_data(dataset, network, weights_file, train_dl, ts_dl, dm_dl):\n",
    "    if dataset == 'test' or dataset == 'train':\n",
    "        data_loader = train_dl\n",
    "    if dataset == 'timesplit':\n",
    "        data_loader = ts_dl\n",
    "    if dataset == 'drugmatrix':\n",
    "        data_loader = dm_dl\n",
    "\n",
    "    network_target_map = load_target_map(train_dl.target_map_file)\n",
    "\n",
    "    if dataset == 'train':\n",
    "        km = data_loader.get_known_mask(data_loader.train_indices)\n",
    "        inds = data_loader.train_indices\n",
    "    elif dataset == 'test':\n",
    "        km = data_loader.get_known_mask(data_loader.test_indices)\n",
    "        inds = data_loader.test_indices\n",
    "    elif dataset == 'timesplit':\n",
    "        km = data_loader.get_known_mask(np.arange(len(data_loader.all_pos), dtype=int))\n",
    "        inds = None\n",
    "    elif dataset == 'drugmatrix': \n",
    "        known_target_slice, _ = align_target_maps(data_loader.target_map, train_dl.target_map)\n",
    "        km = data_loader.get_known_mask(np.arange(len(data_loader.fingerprints), dtype=int))\n",
    "        km = km[known_target_slice]\n",
    "        inds = None\n",
    "    predictions, knowns = get_predictions_of_knowns(data_loader=data_loader,\n",
    "                                                    weights_filename=weights_file,\n",
    "                                                    indices=inds,\n",
    "                                                    network=network,\n",
    "                                                    network_target_map=network_target_map)    \n",
    "\n",
    "    # unravel and save predictions\n",
    "    pred_matrix = np.zeros(km.shape)\n",
    "    pred_matrix[:] = np.nan\n",
    "    pred_matrix[km] = predictions\n",
    "\n",
    "    # unravel and save knowns\n",
    "    known_matrix = np.zeros(km.shape)\n",
    "    known_matrix[:] = np.nan\n",
    "    known_matrix[km] = knowns\n",
    "\n",
    "    return pred_matrix, known_matrix\n",
    "    \n",
    "class  Experiment(dict):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.folds = []\n",
    "        self.converged_epochs = []\n",
    "        self.trained_paths = []\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return str(vars(self))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return json.dumps(vars(self), indent=2)\n",
    "        \n",
    "    def set_converged_epoch(self, epoch, train_path, fold=None):        \n",
    "        self.folds.append(fold)\n",
    "        self.converged_epochs.append(epoch)\n",
    "        self.trained_paths.append(train_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_network_script_from_train_path(train_path, network_script_fmter):\n",
    "    script_name = train_path.split(\"trained_nets/\")[-1].split(\"/\")[0]\n",
    "    return network_script_fmter.format(script_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_BASE = get_env_var(\"DATA_SAVE_BASE\")\n",
    "outdir = \"{}/20190410_SMA_Investigation/predictions/{}\".format(SAVE_BASE, \"{}\")\n",
    "expt_base = outdir.format(\"STD_SMA_RATIOS\")\n",
    "converged_epochs = \"{}/experiments.json\".format(expt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "srv_save_dir = get_env_var(\"DATA_SAVE_BASE\")\n",
    "new_timesplit_dir = '{}/20180525_DM_scrubbing/train_data/'.format(srv_save_dir)\n",
    "new_timesplit_train = os.path.join(new_timesplit_dir, 'train_ts2012_chembl20_MWmax800_scrubDM_minpos10_cutoff5.hdf5')\n",
    "new_timesplit_val = os.path.join(new_timesplit_dir, 'val_ts2012_chembl20_MWmax800_scrubDM_minpos10_cutoff5.hdf5')\n",
    "new_timesplit_map = os.path.join(new_timesplit_dir, 'ts2012_chembl20_MWmax800_scrubDM_minpos10_cutoff5_target_index.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(converged_epochs, \"r\") as fp:\n",
    "    expts = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/ecaceres/labgits/neural-nets/common/chembl_export_data_loader.py:101: FutureWarning: sortlevel is deprecated, use sort_index(level= ...)\n",
      "  tdf.sortlevel(['target', 'compound'], inplace=True)\n",
      "/srv/home/ecaceres/anaconda2/envs/features/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "network_script_fmter = \"{}/labgits/neural-nets/experiments/{}.py\".format(get_env_var(\"HOME\"), \"{}\")\n",
    "\n",
    "ts_dl = H5pyDataLoader(\n",
    "    hdf5_file=new_timesplit_val,\n",
    "    target_map_file=new_timesplit_map, \n",
    "    train_percentage=None, multitask=True)\n",
    "ts_dl.load_training_data()\n",
    "dm_dl = DrugMatrixDataLoader()\n",
    "train_dl = H5pyDataLoader(hdf5_file=new_timesplit_train,\n",
    "                          target_map_file=new_timesplit_map, \n",
    "                          train_percentage=None, \n",
    "                          multitask=True)\n",
    "\n",
    "datasets = ['test', 'train', 'timesplit', 'drugmatrix']\n",
    "\n",
    "for e in expts:\n",
    "    first = True\n",
    "    for epoch, path, fold in izip(e[\"converged_epochs\"], e[\"trained_paths\"], e[\"folds\"]):\n",
    "        # epoch network info\n",
    "        network_script = get_network_script_from_train_path(path, network_script_fmter)\n",
    "        test_index_file = \"{}/test_indices.npy\".format(path)\n",
    "        train_dl.test_indices_file = test_index_file\n",
    "        weights_f = os.path.join(path, \"model_at_epoch_{}.npz\".format(epoch))\n",
    "        network = get_network_from_weights(weights_f, build_nn=network_script)\n",
    "        \n",
    "        # this should update the indices each time.\n",
    "        train_dl.train_indices, train_dl.test_indices = train_dl.get_train_test_indices()\n",
    "        train_dl.load_training_data()\n",
    "        # get data ready for predictions\n",
    "        train_known = train_dl.all_act[train_dl.train_indices]\n",
    "        test_known = train_dl.all_act[train_dl.test_indices]\n",
    "        # n_molecules shouldn't change\n",
    "        assert(train_known.shape[0] + test_known.shape[0] == train_dl.all_act.shape[0])\n",
    "        # make predictions\n",
    "        for ds in datasets:\n",
    "            preds, knowns = predictions_knowns_from_trained_network_and_data(ds, network, weights_f, train_dl, ts_dl, dm_dl)\n",
    "            predf = os.path.join(outdir, '{}_{}_{}_regression_preds.npz'.format(e[\"name\"], ds, fold))\n",
    "            knwnf = os.path.join(outdir, '{}_{}_{}_regression_knowns.npz'.format(e[\"name\"], ds, fold))\n",
    "            \n",
    "            break\n",
    "        break\n",
    "#             np.savez_compressed(predf, preds)\n",
    "#             np.savez_compressed(knwnf, knowns)\n",
    "            \n",
    "#         # save targets to file\n",
    "#         if first: \n",
    "#             np.savez('{}/targets/ValTrain_targets.npz'.format(outdir), load_target_list(train_dl.target_map_file))\n",
    "#             # these two should map to the same protein targets\n",
    "#             dm_target_slice, train_target_slice = align_target_maps(dm_dl.target_map, train_dl.target_map)\n",
    "#             np.savez('{}/targets/drugmatrix_targets.npz'.format(outdir), train_dl.target_map[train_target_slice[-1]])\n",
    "#             np.savez('{}/targets/timesplit_targets.npz'.format(outdir), load_target_list(ts_dl.target_map_file))\n",
    "#             first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm_target_slice, train_target_slice = align_target_maps(dm_dl.target_map, train_dl.target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-438af2939a47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_target_slice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "train_dl.target_map[train_target_slice[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-34554e4f0360>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-34554e4f0360>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tm_lookup = {v,k for k,v in train_dl.target_map}\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tm_lookup = {v,k for k,v in train_dl.target_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
