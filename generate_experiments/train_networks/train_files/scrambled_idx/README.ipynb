{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info see https://github.com/keiserlab/lab-notebook-caceres/wiki/20180815_Paper_Retrains_scrambled_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this experiment is to create an artificial baseline for neural net training on scrambled indices. I will scramble the training set from lab-notebook-caceres/Projects/nnets/20180613_ChEMBL_only_runs and then evaluate performance on the validation, Drug Matrix, and Time Split holds outs to provide baseline measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function shuffle:\n",
      "\n",
      "shuffle(...)\n",
      "    shuffle(x)\n",
      "    \n",
      "    Modify a sequence in-place by shuffling its contents.\n",
      "    \n",
      "    This function only shuffles the array along the first axis of a\n",
      "    multi-dimensional array. The order of sub-arrays is changed but\n",
      "    their contents remains the same.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        The array or list to be shuffled.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> arr = np.arange(10)\n",
      "    >>> np.random.shuffle(arr)\n",
      "    >>> arr\n",
      "    [1 7 5 2 9 4 3 6 0 8]\n",
      "    \n",
      "    Multi-dimensional arrays are only shuffled along the first axis:\n",
      "    \n",
      "    >>> arr = np.arange(9).reshape((3, 3))\n",
      "    >>> np.random.shuffle(arr)\n",
      "    >>> arr\n",
      "    array([[3, 4, 5],\n",
      "           [6, 7, 8],\n",
      "           [0, 1, 2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create array\n",
    "a = np.asarray([[1,1,1,1], [2,2,2,2], [3,3,3,3],[4,4,4,4], [5,5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [2, 2, 2, 2],\n",
       "       [3, 3, 3, 3],\n",
       "       [4, 4, 4, 4],\n",
       "       [5, 5, 5, 5]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use np.random.shuffle to randomly change the rows\n",
    "np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, 3],\n",
       "       [2, 2, 2, 2],\n",
       "       [1, 1, 1, 1],\n",
       "       [5, 5, 5, 5],\n",
       "       [4, 4, 4, 4]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scramble hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_env_var(handle):\n",
    "    tmp = os.getenv(handle)\n",
    "    if not tmp:\n",
    "        raise LookupError(\"Environment variable: {} not set.\".format(handle))\n",
    "    return tmp.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_base = get_env_var(\"DATA_SAVE_BASE\")\n",
    "output_base = get_env_var(\"HOME_SAVE_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = \"{}/20180525_DM_scrubbing/train_data/train_ts2012_chembl20_MWmax800_scrubDM_minpos10_cutoff5.hdf5\".format(input_base)\n",
    "output_file = \"{}/output/20180815_Paper_Retrains/scrambled_idx/SCRAMBLED_train_ts2012_chembl20_MWmax800_scrubDM_minpos10_cutoff5.hdf5\".format(output_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training cases: 296190\n",
      "Fingerprint length: 4096\n",
      "Number of targets: 2038\n",
      "[u'activity', u'fp_array', u'position', u'relation', u'year']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(input_file, 'r') as f:\n",
    "    num_training_cases = f.attrs['training_cases']\n",
    "    fp_len = f.attrs['fprint_len']\n",
    "    num_targets = f.attrs['num_targets']\n",
    "    num_training_cases, fp_len = f['fp_array'].shape\n",
    "    print(\"Number of training cases: %d\" % num_training_cases)\n",
    "    print(\"Fingerprint length: %d\" % fp_len)\n",
    "    print(\"Number of targets: %d\" % num_targets)\n",
    "    print([i for i in f.iterkeys()])\n",
    "    acts = f['activity'][()].copy()\n",
    "    pos = f['position'][()].copy()\n",
    "    fps = f['fp_array'][()].copy()\n",
    "    rels = f['relation'][()].copy()\n",
    "    years = f['year'][()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 40, 44, ..., 53, 55, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 26, 55, ..., 50, 61, 53])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(output_file, 'w') as f:\n",
    "        fp_arr = f.create_dataset('fp_array', fps.shape, dtype=np.bool, chunks=True, fillvalue=False, compression=\"lzf\")\n",
    "        act_arr = f.create_dataset('activity', acts.shape, dtype=np.float32, chunks=True, fillvalue=0.0, compression=\"lzf\")\n",
    "        pos_arr = f.create_dataset('position', pos.shape, dtype=np.uint16, chunks=True, fillvalue=0, compression=\"lzf\")\n",
    "        rel_arr = f.create_dataset('relation', rels.shape, dtype=\"S1\", chunks=True, fillvalue='', compression=\"lzf\")\n",
    "        year_arr = f.create_dataset('year', years.shape, dtype=np.uint16, chunks=True, fillvalue=0, compression=\"lzf\")\n",
    "        \n",
    "        # set values\n",
    "        fp_arr[:] = fps\n",
    "        act_arr[:] = acts\n",
    "        pos_arr[:] = pos\n",
    "        rel_arr[:] = rels\n",
    "        year_arr[:] = years\n",
    "\n",
    "        # declare attributes for meta-data\n",
    "        f.attrs[\"activity_units\"] = \"nM, median\"\n",
    "        f.attrs[\"relationship_type\"] = \"mode\"\n",
    "        f.attrs[\"year_type\"]=\"First publication date. If None given, value = 0\"\n",
    "        f.attrs[\"training_cases\"] = num_training_cases\n",
    "        f.attrs[\"num_targets\"] = num_targets \n",
    "        f.attrs[\"fprint_len\"] = fp_len\n",
    "        f.attrs[\"fprint_type\"] = \"bit/ECFP4\"\n",
    "        f.attrs[\"desc\"] = \"Scrambled Training data for ECFP multi-task network with DM scrubbed and no PCBA. 10 positive ligands/target with a cutoff of pac50 of 5.0.  See lookup tables for target indexing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### On mk-gpu-1, I ran 5-fold cross validation with a 1.0 pnr\n",
    "```\n",
    "./scrambled_trains.sh\n",
    "```\n",
    "\n",
    "### Then, I ran get_metrics to get the best performing epoch\n",
    "```\n",
    "./get_metrics.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run get_best_epochs.sh to get best epoch then change experiments.json to agree with visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import json\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_env_var(handle):\n",
    "    ''' Get an environment variable given the handle for the bash variable'''\n",
    "    tmp = os.getenv(handle)\n",
    "    if not tmp:\n",
    "        raise LookupError(\"Environment variable: {} not set.\".format(handle))\n",
    "    return tmp.strip(\"'\")\n",
    "\n",
    "def format_md_img(link_name, rel_plot_loc):\n",
    "    img_formatter=\"[[{}/{}]]\".format(link_name, rel_plot_loc)\n",
    "    return img_formatter\n",
    "\n",
    "def print_table(list_of_headers):\n",
    "    fmter =  \"| {} {}\".format(\"{}\", \" {} \")\n",
    "    for header in list_of_headers:\n",
    "        fmter = fmter.format(\"{} {}\".format(header, \"| {}\"), \"{}\")\n",
    "    fmter = fmter.format(\" \", \" \")\n",
    "    return fmter\n",
    "\n",
    "class  Experiment(dict):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.folds = []\n",
    "        self.converged_epochs = []\n",
    "        self.trained_paths = []\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return str(vars(self))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return json.dumps(vars(self), indent=2)\n",
    "        \n",
    "    def set_convereged_epoch(self, epoch, train_path, fold=None):        \n",
    "        self.folds.append(fold)\n",
    "        self.converged_epochs.append(epoch)\n",
    "        self.trained_paths.append(train_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home=get_env_var(\"HOME\")\n",
    "base=\"{}/labgits/lab-notebook-caceres.wiki/images/20180815_Paper_Retrains/\".format(home)\n",
    "\n",
    "home_save_dir = get_env_var(\"HOME_SAVE_BASE\")\n",
    "srv_save_dir = get_env_var(\"DATA_SAVE_BASE\")\n",
    "proj_dir = get_env_var(\"NMEW_PROJ_BASE\")\n",
    "\n",
    "sneg_pnrs=[1.0]\n",
    "fold_tmplt = \"fold_[0-9]*/pnr_*/*.png\"\n",
    "\n",
    "# github formatting info:\n",
    "github_wiki_link=\"https://github.com/keiserlab/lab-notebook-caceres/wiki/images\"\n",
    "expt_name = \"20180815_Paper_Retrains\"\n",
    "expt_sub_name=\"scrambled_idx\"\n",
    "github_wiki_expt = \"{}/{}\".format(github_wiki_link, expt_name, expt_sub_name)\n",
    "\n",
    "expt_dir = \"{}/lr_nesterov_1024_2048_3072/\".format(base)\n",
    "\n",
    "metrics_for_convergance = ['matthews-corrcoef_binary-5.0_test', 'matthews-corrcoef_binary-6.0_test', 'r2_test']\n",
    "experiments = []\n",
    "\n",
    "parent_dir = os.path.join(expt_dir, expt_sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copied images of interest into parent dir\n",
    "\n",
    "```\n",
    "!rsync -azrRv $HOME_SAVE_BASE/output/20180815_Paper_Retrains/trained_nets/./lr_nesterov_1024_2048_3072/scrambled_idx/fold_*/pnr_*/*.png $base\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "png_files = glob(os.path.join(parent_dir, fold_tmplt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiments = []\n",
    "converged_epochs = (\"{}/output/20180815_Paper_Retrains/predictions/{}/experiments.json\".format(home_save_dir, expt_sub_name))\n",
    "with open(converged_epochs, \"r\") as fp:\n",
    "    expts = json.load(fp)\n",
    "\n",
    "expt_list = []\n",
    "for e in expts:\n",
    "    tmp = Experiment(e[\"name\"])\n",
    "    for epoch, path, fold in izip(e[\"converged_epochs\"], e[\"trained_paths\"], e[\"folds\"]):\n",
    "        tmp.set_convereged_epoch(epoch, path, fold)\n",
    "    experiments.append(tmp)\n",
    "    del(tmp)    \n",
    "    \n",
    "expt_dict = {float(i.name.split(\"_\")[-1]): dict(zip([int(j.split(\"_\")[1]) for j in i.folds], i.converged_epochs)) for i in experiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: {0: 9, 1: 35, 2: 13, 3: 26, 4: 32}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| PNR_1.0 | train_loss | test_loss | test_no_gt_loss | test_sneg_loss |      \n",
      "| :--- | :--- | :--- | :--- | :--- |      \n",
      "| fold_0 best epoch: 9 | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_0/pnr_1.0/train_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_0/pnr_1.0/test_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_0/pnr_1.0/test_no_gt_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_0/pnr_1.0/test_sneg_loss.png]] |      \n",
      "| fold_1 best epoch: 35 | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_1/pnr_1.0/train_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_1/pnr_1.0/test_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_1/pnr_1.0/test_no_gt_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_1/pnr_1.0/test_sneg_loss.png]] |      \n",
      "| fold_2 best epoch: 13 | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_2/pnr_1.0/train_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_2/pnr_1.0/test_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_2/pnr_1.0/test_no_gt_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_2/pnr_1.0/test_sneg_loss.png]] |      \n",
      "| fold_3 best epoch: 26 | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_3/pnr_1.0/train_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_3/pnr_1.0/test_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_3/pnr_1.0/test_no_gt_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_3/pnr_1.0/test_sneg_loss.png]] |      \n",
      "| fold_4 best epoch: 32 | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_4/pnr_1.0/train_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_4/pnr_1.0/test_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_4/pnr_1.0/test_no_gt_loss.png]] | [[https://github.com/keiserlab/lab-notebook-caceres/wiki/images/20180815_Paper_Retrains/lr_nesterov_1024_2048_3072/scrambled_idx/fold_4/pnr_1.0/test_sneg_loss.png]] |      \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_order = ['train_loss','test_loss', 'test_no_gt_loss', 'test_sneg_loss']\n",
    "for ratio in sneg_pnrs:\n",
    "    png_subset = [f for f in png_files if str(ratio) in f]\n",
    "    headers = [\"PNR_{}\".format(ratio)]\n",
    "    headers.extend(img_order)\n",
    "    print(print_table(headers))\n",
    "    print(print_table([\":---\"]*len(headers)))\n",
    "    for f in np.arange(0, 5, 1):\n",
    "        name_fmter = \"fold_{} best epoch: {}\".format(str(f), expt_dict[ratio][f])\n",
    "        fold_fmter = \"fold_{}\".format(str(f))\n",
    "        fold_pngs = [i for i in png_subset if fold_fmter in i]\n",
    "        fold_pngs = sorted(fold_pngs, key=lambda x: img_order.index(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "        md_print_fmt = [name_fmter] + [format_md_img(github_wiki_expt, i.split(\"//\")[-1]) for i in fold_pngs]\n",
    "        print(print_table(md_print_fmt))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since the images show poor performance on the test set (expected), I just choose the epoch for the .json files to be the same as the default for PNR training: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ../STD_SMA_RATIOS/plot_fcns.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
